---
layout: epic
title: "Augmenting Artworks: AR at Artsy"
date: 2018-03-18
author: orta
categories: [native, ios, arkit]
# css: what-is-react-native
# comment_id: 420
---

In 2017 Apple released ARKit, to universal acclaim. It's a solid foundation for application developers to build
Augmented Reality (AR) experiences without learning a whole new skillset in computer vision. Like a lot of Apple's
technology, it's a clever blend of existing projects: SceneKit, CoreMotion, CoreML and some very clever camera work.
From the developer's perspective it has an API which fits perfectly with the rest of Apple's APIs. You spend most of
your time working with a few delegate functions and you're off.

For the last 2 months, I've been working on a replacement for our View in Room feature on modern devices to support a
"View in My Room" instead. I'm going to try cover how we approached the project, the abstractions we created and give
you a run through of how it works.

<!-- more -->

<div class="mobile-only">
<p>
  <strong>Before you get started</strong>, it looks like you're using a really small screen, 
  this post is built with larger screens in mind.</strong> You will be missing sections otherwise.
</p>
</div>

## How does ARKit work?

You start up ARKit by creating an `ARSession` and passing it to an `ARSCNView`. This then starts looking through the
phone's camera for reference points to determine where the phone is in the world. Your app will use the reference points
to attach SceneKit objectstyle s to places through the AR viewport. Let's look at this as a diagram:

<article class="desktop-only" style='display: flex; flex-flow:row; margin: 0px -60px;'>
<div style='flex:1;'>
  <ol class="workflow" data-id="What-is-AR">
    <li>You have a phone in a room</li>
    <li>ARSession starts</li>
    <li>It detects some feature points</li>
    <li>You can then attach objects to those points</li>
  </ul>
</div>
<div style='flex:1'>{% include svg/ar/what-is-ar.svg %}</div>
</article>

With ARKit at release, if enough feature points are detected in a series horizontally, then ARKit will also send you
some information about the what it considers to be a horizontal plane.

Planes are what most AR apps use on iOS, the apps react to discovering a flat horizontal surface and then allows putting
3d objects on to it. You would then interact with these object by doing a hit test for the closest plane or feature
point to handle user interactions.

## AR at Artsy

We did some interesting work in the AR space already in 2017 with Microsoft, Studio Drift and the Armory Show using [the
Holo Lens][holo-lens]. When ARKit came out we explored whether we could reliably place an Artwork on a wall, but came to
the conclusion that the technology was a bit too immature for us to build it reliably. The core problem was that ARKit
at release could only be used to find horizontal planes.

Here's how our original approach worked:

<article class="desktop-only" style='display: flex; flex-flow:row; margin: 0px -60px;'>
<div style='flex:1;'>
  <ol class="workflow" data-id="v1">
    <li>Open up ARKit and let it discover the world</li>
    <li>Detect the edge of a floor by asking a user to point at the floor near the wall</li>
    <li>Find the edge furthest away</li>
    <li>Attach a work to the floor closest to the wall</li>
  </ul>
</div>
<div style='flex:1'>{% include svg/ar/v1.svg %}</div>
</article>

This could work, but it was hard to explain for a first time user. A user spent most of their time with their phone
pointed at the floor, then we had no way to know if feature points had been detected all the way to the floor edges.

From there, we moved on experimented with using the iPhone's location as a point of reference:

<article class="desktop-only" style='display: flex; flex-flow:row; margin: 0px -60px;'>
<div style='flex:1;'>
  <ol class="workflow" data-id="v2">
  <li>Open up ARKit and let it discover the world</li>
  <li>Ask a user to put their phone against the wall</li>
  <li>Detect that ARKit has disconnected, use the last known point as an anchor</li>
  <li>When the user pulls back and reconnects, place the work</li>
  </ul>
</div>
<div style='flex:1'>{% include svg/ar/v2.svg %}</div>
</article>

We got this to a point where it was working most of the time. One of the core issues is that their is a lot of
guess-work at the end of the process. The Artwork would usually be at the position of your phone, but it was rarely
oriented correctly. The positioning of the Artwork was tough in this case. We experimented by placing the artwork a
meter away, but in user testing very few people could do that roughly in their head, and opted for the exact position of
the phone - which meant that you had to touch the wall, then come back to see if it fit right.

The user flow was hard to explain, and it felt like we weren't setting people up to win. It was quite a debate about
whether we could ship with this or not. Then we read about the upcoming changes in ARKit 1.5. **Vertical plane
detection**. Or, _mostly_ wall detection.

With the advent of vertical wall detection, we could change our flow to be a lot like what someone would have thought it
would be:

<article class="desktop-only" style='display: flex; flex-flow:row; margin: 0px -60px;'>
<div style='flex:1;'>
  <ol class="workflow" data-id="v3">
    <li>Open up ARKit and let it discover the world</li>
    <li>Ask a user to point at a wall</li>
    <li>Use the feature detection to detect a plane</li>
    <li>Use that plane to place an Artwork</li>
  </ul>
</div>
<div style='flex:1'>{% include svg/ar/v3.svg %}</div>
</article>

This is almost perfect, the only flaw is that this example does work in principal. You don't get enough feature points
on a blank wall, so recommending user point at a wall is likely to end up in furstration. So instead we have it more
like:

<article class="desktop-only" style='display: flex; flex-flow:row; margin: 0px -60px;'>
<div style='flex:1;'>
  <ol class="workflow" data-id="v4">
    <li>Open up ARKit and let it discover the world</li>
    <li>Ask a user to point at something on a wall</li>
    <li>Expand that plane in all directions</li>
    <li>Use the extended plane to place an Artwork</li>
  </ul>
</div>
<div style='flex:1'>{% include svg/ar/v4.svg %}</div>
</article>

:tada: - you have the Artsy workflow for putting a work on the wall.

## Abstractions

While we were not sure about how well ARKit would work out for us, we thought the best way to get our feet wet would be
to work with a contractor. We knew [@lazerwalker][lazerwalker] through the iOS dev community and asked if he was
interested in prototyping it out. He built out a demo application of the first design iteration, you can [see it
here][prototype1] as it's MIT licensed.

I took his prototype and [merged it into our app][ar_pr_1] Eigen. From there I started to turn the project into
production code. We effectively had two state machines, one that was handling the internal state of the AR world and
another which handled the user interface from UIKit world. We kept these in sync by having two protocols:
`ARVIRInteractive` and `ARVIRDelegate`.

They didn't end up being that complex, you can see them in [`ARAugmentedVIRInteractionController.h`][vir_ic]. In fact,
the story of AR at Artsy is that most of this is not too complex. Which I think is a testamount to how well thought-out
ARKit is.

So to make it work, we have a reasonably complicated UIViewController subclass,
[`ARAugmentedVIRViewController.m`][vir_vc] which handles presenting a user interface above the camera, then an
interaction controller [`ARAugmentedVIRInteractionController.m`][vir_ic_m] which handles all of the interactions inside
AR. By using protocols to communicate between two, I could test out different AR interactions by duplicating the
existing interaction controller and iterating on a new object for a while. This made it possible to prototype a few
ideas, then switch between them - saving on dev time as this project wasn't a good fit for [React Native][rn].

Testing this code turned out to not be too valuable, I have tests that [cover a lot of the UIKit level][tests] work but
I avoided writing tests for ARKit code. It was changing too often, and was often exploritory. Also, our tests run on iOS
10, which doesn't support ARKit anyway. So those tests would not run on CI.

We explicitly aimed to try and have this project as free as possible from dependencies so that others could re-use this
code. It's not _quite_ generic enough to warrant making it's own CocoaPod, but it's MIT licensed and most of the
dependencies are to do with styling and testing, which should be easily replaced in another codebase. Let us know if
you've explored adding something it to your app!

## Demo

Wonder what it looks like in production? Click through for a video of it in action in my tiny manhattan bedroom:

<center>
<a href="http://files.artsy.net/videos/eigen-arvir-demo.mp4">
<img src="/images/ar/ar-vir-demo.png" width=325>
</a>
</center>

 <style type="text/css">
.workflow li {
  width: 230px;
  padding: 20px;
  list-style:none;
  color: #80D7DA;
  border: 1px solid #80D7DA;
  margin-top:-1px;
}
.workflow li.active {
  border-left: 1px solid black;
  border-right: 1px solid black;
  color: black;
}
</style>

<script>
  $(() => {
    $(".workflow li").hover(function(){
        var thisIndex = $(this).index()  + 1
        var parentDataID = $(this).parent().data("id")
        $(this).parent().children().removeClass('active')
        $(this).addClass('active')

        $('g#' + parentDataID +' g').toArray().forEach(g => {
          if (!g.id.includes("-")) { return }

          var before = parseInt(g.id.split("-")[0])
          var after = parseInt(g.id.split("-")[1].split("_")[0])
          var shouldShow = before <= thisIndex && thisIndex <= after

          g.style.display = shouldShow ? "block" : "none"
        })
    });

    $(".workflow li:first-child").mouseenter()
}) </script>

[holo-lens]: http://fortune.com/2017/03/02/microsoft-hololens-art-show/
[lazerwalker]: http://www.lazerwalker.com
[rn]: http://artsy.github.io/series/react-native-at-artsy/
[prototype1]: https://github.com/lazerwalker/art-on-walls-prototype
[ar_pr_1]: https://github.com/artsy/eigen/pull/2501
[vir_ic]: https://github.com/artsy/eigen/blob/master/Artsy/View_Controllers/ARVIR/ARAugmentedVIRInteractionController.h
[vir_ic_m]: https://github.com/artsy/eigen/blob/master/Artsy/View_Controllers/ARVIR/ARAugmentedVIRInteractionController.m
[vir_vc]: https://github.com/artsy/eigen/blob/master/Artsy/View_Controllers/ARVIR/ARAugmentedVIRViewController.m
[tests]: https://github.com/artsy/eigen/tree/master/Artsy_Tests/View_Controller_Tests/ARVIR
